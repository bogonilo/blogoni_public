list installed kernel packages:

# sudo dpkg -l | grep linux-image

restart login system

# sudo systemctl restart systemd-logind



PYTHON


setup python env

# python -m venv /path/to/venv

version specific 

# python3.11 -m venv llama/venv/


activate venv

# source /venv/bin/activate


OLLAMA 

# sudo systemctl start ollama
# sudo systemctl status ollama
# ollama serve


# export INFERENCE_MODEL="meta-llama/Llama-3.1-8B"
ollama names this model differently, and we must use the ollama name when loading the model
# export OLLAMA_INFERENCE_MODEL="llama3.1:8b"

This must be run in its own terminal
# sudo ollama serve
Then in another terminal
# sudo ollama run $OLLAMA_INFERENCE_MODEL --keepalive 60m


# curl http://localhost:11434/api/chat -d '{
  "model": "llama3.1:8b",
  "messages": [
    {
      "role": "user",
      "content": "who wrote the book godfather?"
    }
  ],
  "stream": true
}'

curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2:3b",
  "messages": [
    {
      "role": "user",
      "content": "who wrote the book godfather?"
    }
  ],
  "stream": true
}'



DOCKER

run docker

# export LLAMA_STACK_PORT=5001
# sudo docker run \
  -it \
  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \
  -v ~/.llama:/root/.llama \
  llamastack/distribution-ollama \
  --port $LLAMA_STACK_PORT \
  --env INFERENCE_MODEL=$INFERENCE_MODEL \
  --env OLLAMA_URL=http://host.docker.internal:11434

# LLAMA_STACK_PORT=5001
# sudo docker run \
  -it \
  -p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT \
  -v ~/.llama:/root/.llama \
  llamastack/distribution-meta-reference-gpu \
  --port $LLAMA_STACK_PORT \
  --env INFERENCE_MODEL=meta-llama/Llama-3.1-8B-Instruct
